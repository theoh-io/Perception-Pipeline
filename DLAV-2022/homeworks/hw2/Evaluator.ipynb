{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Evaluator.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"fPFXjAVFIKnh"},"outputs":[],"source":["import argparse\n","import numpy as np\n","import torch\n","import torch.nn.functional as F\n","import pickle\n","import platform\n","import os"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"uGxnwhvlwMiI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!git clone https://github.com/vita-epfl/DLAV-2022.git\n","path = os.getcwd() + '/DLAV-2022/homeworks/hw2/test_batch'"],"metadata":{"id":"SwxcJW9wI9fp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Write the location of the saved weight relative to this notebook. Assume that they are in the same directory\n","### Path to Model Weights \n","softmax_weights = .... \n","pytorch_weights = ...."],"metadata":{"id":"pZXQTJIKJE_S"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**TODO:** Copy your code from the Softmax Notebook to their corresponding function"],"metadata":{"id":"mE6psT_aVPHv"}},{"cell_type":"code","source":["\n","def softmax_loss_vectorized(W, X, y):\n","    \"\"\"\n","  Softmax loss function, vectorized version.\n","  Inputs and outputs are the same as softmax_loss_naive.\n","  \"\"\"\n","    # Initialize the loss and gradient to zero.\n","    loss = 0.0\n","    dW = np.zeros_like(W)\n","\n","    #############################################################################\n","    # TODO: Compute the softmax loss and its gradient using no explicit loops.  #\n","    # Store the loss in loss and the gradient in dW. If you are not careful     #\n","    # here, it is easy to run into numeric instability. Don't forget the        #\n","    # regularization!                                                           #\n","    #############################################################################\n","    pass\n","    #############################################################################\n","    #                          END OF YOUR CODE                                 #\n","    #############################################################################\n","    \n","    return loss, dW\n","\n","class LinearClassifier(object):\n","\n","    def __init__(self):\n","        self.W = None\n","\n","\n","    def train(self, X, y, learning_rate=1e-3, num_iters=30000,\n","                batch_size=200, verbose=False):\n","        \"\"\"\n","        Train this linear classifier using stochastic gradient descent.\n","\n","        Inputs:\n","        - X: A numpy array of shape (N, D) containing training data; there are N\n","          training samples each of dimension D.\n","        - y: A numpy array of shape (N,) containing training labels; y[i] = c\n","          means that X[i] has label 0 <= c < C for C classes.\n","        - learning_rate: (float) learning rate for optimization.\n","        - num_iters: (integer) number of steps to take when optimizing\n","        - batch_size: (integer) number of training examples to use at each step.\n","        - verbose: (boolean) If true, print progress during optimization.\n","\n","        Outputs:\n","        A list containing the value of the loss function at each training iteration.\n","        \"\"\"\n","        \n","        num_train, dim = X.shape\n","        num_classes = np.max(y) + 1 # assume y takes values 0...K-1 where K is number of classes\n","        \n","        if self.W is None:\n","            # lazily initialize W\n","            self.W = 0.001 * np.random.randn(dim, num_classes)\n","\n","        # Run stochastic gradient descent to optimize W\n","        loss_history = []\n","        for it in range(num_iters):\n","            X_batch = None\n","            y_batch = None\n","\n","            #########################################################################\n","            # TODO:                                                                 #\n","            # Sample batch_size elements from the training data and their           #\n","            # corresponding labels to use in this round of gradient descent.        #\n","            # Store the data in X_batch and their corresponding labels in           #\n","            # y_batch; after sampling X_batch should have shape (dim, batch_size)   #\n","            # and y_batch should have shape (batch_size,)                           #\n","            #                                                                       #\n","            # Hint: Use np.random.choice to generate indices. Sampling with         #\n","            # replacement is faster than sampling without replacement.              #\n","            #########################################################################\n","            pass\n","            #########################################################################\n","            #                       END OF YOUR CODE                                #\n","            #########################################################################\n","\n","            # evaluate loss and gradient\n","            loss, grad = self.loss(X_batch, y_batch, reg)\n","            loss_history.append(loss)\n","\n","            # perform parameter update\n","            #########################################################################\n","            # TODO:                                                                 #\n","            # Update the weights using the gradient and the learning rate.          #\n","            #########################################################################\n","            pass\n","            #########################################################################\n","            #                       END OF YOUR CODE                                #\n","            #########################################################################\n","\n","            if verbose and it % 100 == 0:\n","                print('iteration %d / %d: loss %f' % (it, num_iters, loss))\n","\n","\n","        return loss_history\n","    \n","\n","\n","    def predict(self, X):\n","        \"\"\"\n","        Use the trained weights of this linear classifier to predict labels for\n","        data points.\n","\n","        Inputs:\n","        - X: A numpy array of shape (N, D) containing training data; there are N\n","          training samples each of dimension D.\n","\n","        Returns:\n","        - y_pred: Predicted labels for the data in X. y_pred is a 1-dimensional\n","          array of length N, and each element is an integer giving the predicted\n","          class.\n","        \"\"\"\n","\n","        ###########################################################################\n","        # TODO:                                                                   #\n","        # Implement this method. Store the predicted labels in y_pred.            #\n","        ###########################################################################\n","        pass\n","\n","        ###########################################################################\n","        #                           END OF YOUR CODE                              #\n","        ###########################################################################\n","        return y_pred\n","\n","    def loss(self, X_batch, y_batch):\n","        \"\"\"\n","        Compute the loss function and its derivative. \n","        Subclasses will override this.\n","\n","        Inputs:\n","        - X_batch: A numpy array of shape (N, D) containing a minibatch of N\n","          data points; each point has dimension D.\n","        - y_batch: A numpy array of shape (N,) containing labels for the minibatch.\n","\n","\n","        Returns: A tuple containing:\n","        - loss as a single float\n","        - gradient with respect to self.W; an array of the same shape as W\n","        \n","         e = y_batch - np.dot(X_batch, self.W) \n","        \n","        loss = np.dot(e.T, e)\n","        grad = -np.dot(x_batch.T,e) / x_batch.shape[0]\n","  \n","        return loss, grad\n","\n","        \"\"\"\n","\n","        pass\n","        \n","\n","\n","class Softmax(LinearClassifier):\n","    \"\"\" A subclass that uses the Softmax + Cross-entropy loss function \"\"\"\n","\n","    def loss(self, X_batch, y_batch):\n","        return softmax_loss_vectorized(self.W, X_batch, y_batch)"],"metadata":{"id":"gHnLX6-oIkWm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**TODO:** Copy the model you created from the Pytorch Notebook"],"metadata":{"id":"6chaH4G-Vfms"}},{"cell_type":"code","source":["class Net(torch.nn.Module):\n","    def __init__(self, n_feature, n_hidden, n_output):\n","        super(Net, self).__init__()\n","        pass\n","\n","    def forward(self, x):\n","        pass\n","        return x"],"metadata":{"id":"mSTfKTHEJBhy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**TODO**: Follow the instructions in each of the following methods. **Note that these methods should return a 1-D array of size N where N is the number of data samples. The values should be the predicted classes [0,...,9].**\n","\n"],"metadata":{"id":"_UUbNTUAVsos"}},{"cell_type":"code","source":["def predict_usingPytorch(X):\n","    #########################################################################\n","    # TODO:                                                                 #\n","    # - Create your model                                                   #\n","    # - Load your saved model                                               #\n","    # - Do the operation required to get the predictions                    #\n","    # - Return predictions in a numpy array (hint: return \"argmax\")         #\n","    #########################################################################\n","    pass\n","    #########################################################################\n","    #                       END OF YOUR CODE                                #\n","    #########################################################################\n","    return y_pred.numpy()\n","\n","def predict_usingSoftmax(X):\n","    #########################################################################\n","    # TODO:                                                                 #\n","    # - Load your saved model into the weights of Softmax                   #\n","    # - Do the operation required to get the predictions                    #\n","    # - Return predictions in a numpy array                                 #\n","    #########################################################################\n","    pass\n","    #########################################################################\n","    #                       END OF YOUR CODE                                #\n","    #########################################################################\n","    return y_pred"],"metadata":{"id":"bEKafMuaI4By"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This method loads the test dataset to evaluate the model."],"metadata":{"id":"q8dM8fj39OBP"}},{"cell_type":"code","source":["## Read DATA\n","def load_pickle(f):\n","    version = platform.python_version_tuple()\n","    if version[0] == '2':\n","        return  pickle.load(f)\n","    elif version[0] == '3':\n","        return  pickle.load(f, encoding='latin1')\n","    raise ValueError(\"invalid python version: {}\".format(version))\n","\n","def load_CIFAR_batch(filename):\n","  \"\"\" load single batch of cifar \"\"\"\n","  with open(filename, 'rb') as f:\n","    datadict = load_pickle(f)\n","    X = datadict['data']\n","    Y = datadict['labels']\n","    X = X.reshape(10000, 3, 32, 32).transpose(0,2,3,1).astype(\"float\")\n","    Y = np.array(Y)\n","    return X, Y\n","test_filename = path\n","X,Y = load_CIFAR_batch(test_filename)"],"metadata":{"id":"400u4eZNJAZq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This code snippet prepares the data for the different models. If you modify data manipulation in your notebooks, make sure to include them here. "],"metadata":{"id":"AJ3mBYnx9TIe"}},{"cell_type":"code","source":["## Data Manipulation\n","\n","mean = np.array([0.4914, 0.4822, 0.4465])\n","std = np.array([0.2023, 0.1994, 0.2010])\n","X = np.divide(np.subtract( X/255 , mean[np.newaxis,np.newaxis,:]), std[np.newaxis,np.newaxis,:])\n","\n","X_pytorch = torch.Tensor(np.moveaxis(X,-1,1))\n","X_softmax = np.reshape(X, (X.shape[0], -1))\n","X_softmax = np.hstack([X_softmax, np.ones((X_softmax.shape[0], 1))])\n"],"metadata":{"id":"IEmU5KnwJPBY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Runs evaluation on the Pytorch and softmax model. **Be careful that *prediction_pytorch* and *prediction_softmax* are 1-D array of size N where N is the number of data samples. The values should be the predicted class [0,...,9]**\n","\n","---\n","\n"],"metadata":{"id":"O2nQbKPL9c3G"}},{"cell_type":"code","source":["## Run Prediction\n","prediction_pytorch = predict_usingPytorch(X_pytorch)\n","prediction_softmax = predict_usingSoftmax(X_softmax)\n","\n","## Run Evaluation\n","acc_softmax = sum(prediction_softmax == Y)/len(X)\n","acc_pytorch = sum(prediction_pytorch == Y)/len(X)\n","print(\"Softmax= %f ... Pytorch= %f\"%(acc_softmax, acc_pytorch))"],"metadata":{"id":"VKFPhm1wJjDv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"qroI8swROjZf"},"execution_count":null,"outputs":[]}]}