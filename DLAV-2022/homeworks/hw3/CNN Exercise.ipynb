{"cells":[{"cell_type":"markdown","metadata":{"id":"Gb03R-oINjO0"},"source":["## Convolutional Networks\n","\n","We'll check out how to build a **convolutional network** to classify CIFAR10 images. By using weight sharing - multiple units with the same weights - convolutional layers are able to learn repeated patterns in your data. For example, a unit could learn the pattern for an eye, or a face, or lower level features like edges.\n"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"e8ZKW4STOlyI"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H0c3K5CGNjO6"},"outputs":[],"source":["import numpy as np\n","import time\n","\n","import torch\n","from torch import nn\n","from torch import optim\n","import torch.nn.functional as F\n","import torch.utils.data as utils\n","from torchvision import datasets, transforms\n","from torch.utils.data.sampler import SubsetRandomSampler\n","import matplotlib.pyplot as plt\n","%matplotlib inline"]},{"cell_type":"code","source":["label_names = [\n","    'airplane',\n","    'automobile',\n","    'bird',\n","    'cat',\n","    'deer',\n","    'dog',\n","    'frog',\n","    'horse',\n","    'ship',\n","    'truck'\n","]\n","\n","\n","def plot_images(images, cls_true, cls_pred=None):\n","    \"\"\"\n","    Adapted from https://github.com/Hvass-Labs/TensorFlow-Tutorials/\n","    \"\"\"\n","    fig, axes = plt.subplots(3, 3)\n","\n","    for i, ax in enumerate(axes.flat):\n","        # plot img\n","        ax.imshow(images[i, :, :, :], interpolation='spline16')\n","\n","        # show true & predicted classes\n","        cls_true_name = label_names[cls_true[i]]\n","        if cls_pred is None:\n","            xlabel = \"{0} ({1})\".format(cls_true_name, cls_true[i])\n","        else:\n","            cls_pred_name = label_names[cls_pred[i]]\n","            xlabel = \"True: {0}\\nPred: {1}\".format(\n","                cls_true_name, cls_pred_name\n","            )\n","        ax.set_xlabel(xlabel)\n","        ax.set_xticks([])\n","        ax.set_yticks([])\n","\n","    plt.show()"],"metadata":{"id":"kpSL2MzAOWnk"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kYaAPNTtNjO8"},"outputs":[],"source":["def get_train_valid_loader(data_dir='data',\n","                           batch_size=64,\n","                           augment=False,\n","                           random_seed = 1,\n","                           valid_size=0.02,\n","                           shuffle=True,\n","                           show_sample=True,\n","                           num_workers=4,\n","                           pin_memory=False):\n","    \"\"\"\n","    Utility function for loading and returning train and valid\n","    multi-process iterators over the CIFAR-10 dataset. A sample\n","    9x9 grid of the images can be optionally displayed.\n","    If using CUDA, num_workers should be set to 1 and pin_memory to True.\n","    Params\n","    ------\n","    - data_dir: path directory to the dataset.\n","    - batch_size: how many samples per batch to load.\n","    - augment: whether to apply the data augmentation scheme\n","      mentioned in the paper. Only applied on the train split.\n","    - random_seed: fix seed for reproducibility.\n","    - valid_size: percentage split of the training set used for\n","      the validation set. Should be a float in the range [0, 1].\n","    - shuffle: whether to shuffle the train/validation indices.\n","    - show_sample: plot 9x9 sample grid of the dataset.\n","    - num_workers: number of subprocesses to use when loading the dataset.\n","    - pin_memory: whether to copy tensors into CUDA pinned memory. Set it to\n","      True if using GPU.\n","    Returns\n","    -------\n","    - train_loader: training set iterator.\n","    - valid_loader: validation set iterator.\n","    \"\"\"\n","    error_msg = \"[!] valid_size should be in the range [0, 1].\"\n","    assert ((valid_size >= 0) and (valid_size <= 1)), error_msg\n","\n","    normalize = transforms.Normalize(\n","        mean=[0.4914, 0.4822, 0.4465],\n","        std=[0.2023, 0.1994, 0.2010],\n","    )\n","\n","    # define transforms\n","    valid_transform = transforms.Compose([\n","            transforms.ToTensor(),\n","            normalize,\n","    ])\n","    if augment:\n","        train_transform = transforms.Compose([\n","            transforms.RandomCrop(32, padding=4),\n","            transforms.RandomHorizontalFlip(),\n","            transforms.ToTensor(),\n","            normalize,\n","        ])\n","    else:\n","        train_transform = transforms.Compose([\n","            transforms.ToTensor(),\n","            normalize,\n","        ])\n","\n","    # load the dataset\n","    train_dataset = datasets.CIFAR10(\n","        root=data_dir, train=True,\n","        download=True, transform=train_transform,\n","    )\n","\n","    valid_dataset = datasets.CIFAR10(\n","        root=data_dir, train=True,\n","        download=True, transform=valid_transform,\n","    )\n","\n","    num_train = len(train_dataset)\n","    indices = list(range(num_train))\n","    split = int(np.floor(valid_size * num_train))\n","\n","    if shuffle:\n","        np.random.seed(random_seed)\n","        np.random.shuffle(indices)\n","\n","    train_idx, valid_idx = indices[split:], indices[:split]\n","    train_sampler = SubsetRandomSampler(train_idx)\n","    valid_sampler = SubsetRandomSampler(valid_idx)\n","\n","    train_loader = torch.utils.data.DataLoader(\n","        train_dataset, batch_size=batch_size, sampler=train_sampler,\n","        num_workers=num_workers, pin_memory=pin_memory,\n","    )\n","    valid_loader = torch.utils.data.DataLoader(\n","        valid_dataset, batch_size=batch_size, sampler=valid_sampler,\n","        num_workers=num_workers, pin_memory=pin_memory,\n","    )\n","\n","    # visualize some images\n","    if show_sample:\n","        sample_loader = torch.utils.data.DataLoader(\n","            train_dataset, batch_size=9, shuffle=shuffle,\n","            num_workers=num_workers, pin_memory=pin_memory,\n","        )\n","        data_iter = iter(sample_loader)\n","        images, labels = data_iter.next()\n","        X = images.numpy().transpose([0, 2, 3, 1])\n","        plot_images(X, labels)\n","\n","    return (train_loader, valid_loader)\n","\n","trainloader, valloader = get_train_valid_loader()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H1zuJ67-NjO_"},"outputs":[],"source":["class ConvNet(nn.Module):\n","    def __init__(self, n_input_channels=3, n_output=10):\n","        super().__init__()\n","        ################################################################################\n","        # TODO:                                                                        #\n","        # Define 2 or more different layers of the neural network                      #\n","        ################################################################################\n","        pass\n","        ################################################################################\n","        #                              END OF YOUR CODE                                #\n","        ################################################################################\n","    \n","    def forward(self, x):\n","        ################################################################################\n","        # TODO:                                                                        #\n","        # Set up the forward pass that the input data will go through.                 #\n","        # A good activation function betweent the layers is a ReLu function.           #\n","        #                                                                              #\n","        # Note that the output of the last convolution layer should be flattened       #\n","        # before being inputted to the fully connected layer. We can flatten           #\n","        # Tensor `x` with `x.view`.                                                    #\n","        ################################################################################\n","        pass\n","        ################################################################################\n","        #                              END OF YOUR CODE                                #\n","        ################################################################################\n","        \n","        return x\n","    \n","    def predict(self, x):\n","        logits = self.forward(x)\n","        return F.softmax(logits)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AI6ajoIZNjPA"},"outputs":[],"source":["net = ConvNet()\n","################################################################################\n","# TODO:                                                                        #\n","# Choose an Optimizer that will be used to minimize the loss function.         #\n","# Choose a critera that measures the loss                                      #\n","################################################################################\n","criterion = ..............\n","optimizer = ..............\n","\n","epochs = 1\n","steps = 0\n","running_loss = 0\n","print_every = 20\n","for e in range(epochs):\n","    start = time.time()\n","    for images, labels in iter(trainloader):\n","        \n","        steps += 1\n","        ################################################################################\n","        # TODO:                                                                        #\n","        # Run the training process                                                     #\n","        #                                                                              #\n","        #                                                                              #\n","        ################################################################################\n","        pass\n","        ################################################################################\n","        #                              END OF YOUR CODE                                #\n","        ################################################################################\n","        \n","        loss = criterion(output, targets)\n","        ################################################################################\n","        # TODO:                                                                        #\n","        # Run the training process                                                     #\n","        #                                                                              #\n","        # HINT: Calculate the gradient and move one step further                       #\n","        ################################################################################\n","        pass\n","        ################################################################################\n","        #                              END OF YOUR CODE                                #\n","        ################################################################################\n","        \n","        running_loss += loss.item()\n","        \n","        if steps % print_every == 0:\n","            stop = time.time()\n","            # Test accuracy\n","            accuracy = 0\n","            for ii, (images, labels) in enumerate(valloader):\n","                \n","                ################################################################################\n","                # TODO:                                                                        #\n","                # Calculate the accuracy                                                       #\n","                ################################################################################\n","                pass\n","                ################################################################################\n","                #                              END OF YOUR CODE                                #\n","                ################################################################################\n","            \n","            print(\"Epoch: {}/{}..\".format(e+1, epochs),\n","                  \"Loss: {:.4f}..\".format(running_loss/print_every),\n","                  \"Test accuracy: {:.4f}..\".format(accuracy/(ii+1)),\n","                  \"{:.4f} s/batch\".format((stop - start)/print_every)\n","                 )\n","            running_loss = 0\n","            start = time.time()"]},{"cell_type":"markdown","metadata":{"id":"zUqGBbcpNjPB"},"source":["Save best trained model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5BKFS7ALNjPB"},"outputs":[],"source":["## You should be familiar with how to save a pytorch model (Make sure to save the model in your Drive)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.3"},"colab":{"name":"CNN Exercise.ipynb","provenance":[],"collapsed_sections":[]}},"nbformat":4,"nbformat_minor":0}